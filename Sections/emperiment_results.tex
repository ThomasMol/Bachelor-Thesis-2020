This section will provide, analyze and discuss the results of the annotations and of the experiments done with the data-set. First, general statistics will be provided regarding the mood and likeability annotations. Then the new annotations in conjunction with the interview variable will be analyzed to identify any statistical relationship. Next, we will provide the results of the experiments. The performance of the models created during the experiments will be expressed in their Unweighted Average Recall or UAR. The UAR is the mean of the recall scores of each class. The recall is the ratio between the number of true positives and false negatives. If all predictions would be correctly classified the UAR would be 1. The UAR metric  is a better indication of prediction capacity than precision accuracy as our data-set is imbalanced. The UAR does not favor the majority class and is therefore a better indicator of the prediction capabilities of our models.
\break

\subsection{Annotation Analysis}
As described in the methodology, 960 video clips of the data-set were annotated for the Big Five personality traits, two mood primitives (valence and arousal), likeability, background music and the interview variable. The personality and interview dimensions were annotated using Amazon Mechanical Turk. All dimensions, with the exception of background music, were annotated for apparent presence or absence. The interview variable indicates whether the annotator would invite the subject to an interview or not. Table \ref{tab:personcount} and table \ref{tab:goldcount} show the number of annotations for each class and dimension. 

The apparent personality trait and interview dimensions were post-processed to create cardinal scores for each video clip (\cite{escalante2018explaining}). These scores were also binarized by taking the mean of each dimension and assigning a 2 if the score was equal or higher than the mean and assigning a 1 if the score was lower than the mean. Table \ref{tab:personcount} shows the number of annotations based on the binarized data-set. 

\begin{table}[h]
\begin{tabular}{|r|r|r|r|}
\hline
\rowcolor[HTML]{C0C0C0} 
\multicolumn{1}{|l|}{\cellcolor[HTML]{C0C0C0}Class} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}Arousal} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}Valence} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}Likeability} \\ \hline
1                           & 77  & 46  & 99  \\
2                           & 364 & 707 & 505 \\
3                           & 519 & 207 & 356 \\ \hline
\multicolumn{1}{|l|}{Total} & 960 & 960 & 960 \\ \hline
\end{tabular}
\caption{Number of annotations (self) per class of the mood primitive and likeability dimensions}
\label{tab:selfcount}
\end{table}

\begin{table}[h]
\begin{tabular}{|r|r|r|r|}
\hline
\rowcolor[HTML]{C0C0C0} 
\multicolumn{1}{|l|}{\cellcolor[HTML]{C0C0C0}Class} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}Arousal} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}Valence} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}Likeability} \\ \hline
1                           & 108 & 82  & 197 \\
2                           & 593 & 705 & 602 \\
3                           & 259 & 173 & 161 \\ \hline
\multicolumn{1}{|l|}{Total} & 960 & 960 & 960 \\ \hline
\end{tabular}
\caption{Number of annotations (gold min) per class of the mood primitive and likeability dimensions}
\label{tab:goldcount}
\end{table}

\begin{table*}[th]
\begin{tabular}{|r|r|r|r|r|r|r|}
\hline
\rowcolor[HTML]{C0C0C0} Class &
  \multicolumn{1}{l|}{Openness} &
  \multicolumn{1}{P{2cm}|}{Conscientiousness} &
  \multicolumn{1}{l|}{Extraversion} &
  \multicolumn{1}{P{2cm}|}{Agreeableness} &
  \multicolumn{1}{l|}{Neuroticism} &
  \multicolumn{1}{P{2cm}|}{Interview Invitation} \\ \hline
1     & 528 & 458 & 443 & 437 & 441 & 426 \\ \hline
2     & 432 & 502 & 517 & 523 & 519 & 534 \\ \hline
Total & 960 & 960 & 960 & 960 & 960 & 960 \\ \hline
\end{tabular}
\caption{Number of annotations per class of apparent personality trait and interview invitation dimensions}
\label{tab:personcount}
\end{table*}

\subsection{Statistical Relationships}

Some interesting statistical experiments can be done regarding the statistical relationship of the mood primitives and likeability dimensions and the interview invitation. For these experiments we will use the cardinal values of the interview variable and the self annotations of the arousal, valence and likeability dimensions. Three box-plots are created and are shown in figure \ref{fig:boxplotval}, figure \ref{fig:boxplotaro} and figure \ref{fig:boxplotlike}. For these box-plots the y-axis indicates the cardinal interview invitation variable, where a higher score means they would be more likely to be invited for an interview. The x-axis indicate the three ordinal classes of the arousal, valence or likeability dimensions. The orange horizontal line in the box-plots indicate the mean. As the plots show, there is a upwards trend in the mean of all three box-plots. Meaning that if the subject is classified in the higher class, they would be more likely to have a higher score in the interview invitation variable.

\begin{figure}[h]
  \centering
  \includesvg[width=\columnwidth]{Images/boxplot_valence_interview.svg}
  \caption{Box-plot of the valence classes and interview invitation variable}
  \label{fig:boxplotval}
\end{figure}

\begin{figure}[h]
  \centering
  \includesvg[width=\columnwidth]{Images/boxplot_arousal_interview.svg}
  \caption{Box-plot of the arousal classes and interview invitation variable}
  \label{fig:boxplotaro}
\end{figure}

\begin{figure}[h]
  \centering
  \includesvg[width=\columnwidth]{Images/boxplot_likeability_interview.svg}
  \caption{Box-plot of the likeability classes and interview invitation variable}
  \label{fig:boxplotlike}
\end{figure}

However, to see if there is a correlation between the interview variable and mood and likeability variables we must look at the Spearman's Rank-Order Correlation coefficient. This coefficient is used to calculate correlation for ordinal categorical variables. To calculate this coefficient we will be using the binarized data-set of the interview variable. The coefficient is calculated as follows: \(\rho = \frac{cov(x,y)}{\sigma_x \sigma_y}\), where \(cov(x,y)\) is the covariance of the variables and \(\rho\) is the Pearson correlation coefficient. Table \ref{tab:spearman} shows the correlation and p value between the three dimensions and the binarized interview variable. The results show a low p value for all dimensions, with every p value being close to 0. This means that the null hypothesis should be rejected, where the null hypothesis is that two sets of data are uncorrelated. By rejected the null hypothesis we can conclude that the sets of data are not uncorrelated, however it does not prove that they are correlated. If we look at table \ref{tab:spearman} we can see that the correlation values are between 0.2 and 0.4. For the arousal dimension this means that the strength of the correlation is small, while for the valence and likeability dimensions there is medium strong correlation. A perfect correlation would be indicated with a value of 1 or -1 (for a negative correlation). Typically, a correlation is considered strong if the correlation coefficient value is higher than 0.7 or lower than -0.7. 

\begin{table}[h]
\begin{tabular}{|P{1cm}|r|r|r|}
\hline
\rowcolor[HTML]{C0C0C0} 
 & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}Arousal} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}Valence} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}Likeability} \\ \hline
Correlation & 0.296 & 0.204 & 0.395 \\ \hline
P Value & 6.51E-21 & 1.76E-10 & 3.09E-37 \\ \hline
\end{tabular}
\caption{Results of the Spearman Rank Correlation coefficient calculations}
\label{tab:spearman}
\end{table}

\subsection{Mood and Likeability Model Experiments}
In this section we will discuss the performance of the models that are trained using the mood and likeability dimensions and the extracted video features. As stated in the methodology the data-set was split into a training set of 660 rows and a test set of 300 rows. Both the self annotation and the gold standard annotations were used for training the classifiers. Both the SVM and ELM algorithms as described in section \ref{subsection:classificaiton} were used to the train models. Table \ref{tab:lkelm} shows the results in UAR of the trained ELM models. A linear kernel was used to train these models. Also, hyperparameters, like the C parameter, were optimized to achieve the best scores. The highest score of each dimension is highlighted bold. The normalization column refers to what type of normalization was applied to the feature set. "NN" stands for no normalization, "ZN" for z normalization and "L2" stands for l2 normalization. These are the normalization techniques as described in section \ref{subsection:normalization}. As the results show, the best results of each dimension is higher than 33\%. Which indicates that the models are more accurate than chance level. Chance level is 33\% in the case of our data-set as the dimensions are labeled with three different classes. Further, the results for the weighted ELM are higher than non-weighted ELM for all dimensions. Also, the gold standard annotation dimensions score higher than the self annotated labels. This implies that more annotators are needed for the same data to increase the performance of the models. 

\begin{table*}[h]
\begin{tabular}{|l|P{2cm}|r|r|r|r|r|r|}
\hline
\rowcolor[HTML]{C0C0C0} 
\textbf{Weighted} & \textbf{Normalization} & \multicolumn{1}{P{1.5cm}|}{\cellcolor[HTML]{C0C0C0}\textbf{self aro}} & \multicolumn{1}{P{1.5cm}|}{\cellcolor[HTML]{C0C0C0}\textbf{gold aro}} & \multicolumn{1}{P{1.5cm}|}{\cellcolor[HTML]{C0C0C0}\textbf{self val}} & \multicolumn{1}{P{1.5cm}|}{\cellcolor[HTML]{C0C0C0}\textbf{gold val}} & \multicolumn{1}{P{1.5cm}|}{\cellcolor[HTML]{C0C0C0}\textbf{self like}} & \multicolumn{1}{P{1.5cm}|}{\cellcolor[HTML]{C0C0C0}\textbf{gold like}} \\ \hline
\textbf{No} & \textbf{NN} & \cellcolor[HTML]{FFE2E2}39.79\% & \cellcolor[HTML]{FFAEAE}42.67\% & \cellcolor[HTML]{FFF9E7}47.91\% & \cellcolor[HTML]{FFF9E9}47.78\% & \cellcolor[HTML]{FFFFFF}38.47\% & \cellcolor[HTML]{D1EDDF}42.61\% \\ \hline
\textbf{No} & \textbf{ZN} & \cellcolor[HTML]{FF9B9B}43.76\% & \cellcolor[HTML]{FF2E2E}49.88\% & \cellcolor[HTML]{FFF4D5}49.11\% & \cellcolor[HTML]{FFFFFF}46.21\% & \cellcolor[HTML]{6DC499}51.56\% & \cellcolor[HTML]{88CFAD}49.11\% \\ \hline
\textbf{No} & \textbf{L2} & \cellcolor[HTML]{FFE1E1}39.80\% & \cellcolor[HTML]{FFB2B2}42.49\% & \cellcolor[HTML]{FFFAEC}47.53\% & \cellcolor[HTML]{FFFCF1}47.19\% & \cellcolor[HTML]{F0F9F5}39.84\% & \cellcolor[HTML]{CAEADA}43.25\% \\ \hline
\textbf{No} & \textbf{ZN+L2} & \cellcolor[HTML]{FFE4E4}39.68\% & \cellcolor[HTML]{FF5656}47.63\% & \cellcolor[HTML]{FFF1C9}49.95\% & \cellcolor[HTML]{FFFEF9}46.64\% & \cellcolor[HTML]{8DD1B0}48.68\% & \cellcolor[HTML]{7FCBA6}49.98\% \\ \hline
\textbf{Yes} & \textbf{NN} & \cellcolor[HTML]{FFC1C1}41.60\% & \cellcolor[HTML]{FF5F5F}47.15\% & \cellcolor[HTML]{FFF5D7}49.00\% & \cellcolor[HTML]{FFE8A8}52.25\% & \cellcolor[HTML]{70C59B}51.33\% & \cellcolor[HTML]{5CBE8E}53.04\% \\ \hline
\textbf{Yes} & \textbf{ZN} & \cellcolor[HTML]{FF8383}\textbf{45.13\%} & \cellcolor[HTML]{FF0000}\textbf{52.46\%} & \cellcolor[HTML]{FFE9AD}\textbf{51.92\%} & \cellcolor[HTML]{FFDA73}55.92\% & \cellcolor[HTML]{5BBD8D}\textbf{53.19\%} & \cellcolor[HTML]{86CEAB}49.32\% \\ \hline
\textbf{Yes} & \textbf{L2} & \cellcolor[HTML]{FFFFFF}38.11\% & \cellcolor[HTML]{FF1919}51.08\% & \cellcolor[HTML]{FFF3D2}49.36\% & \cellcolor[HTML]{FFD666}\textbf{56.77\%} & \cellcolor[HTML]{66C195}52.18\% & \cellcolor[HTML]{57BB8A}\textbf{53.49\%} \\ \hline
\textbf{Yes} & \textbf{ZN+L2} & \cellcolor[HTML]{FF9898}43.91\% & \cellcolor[HTML]{FF0202}52.37\% & \cellcolor[HTML]{FFF1C8}50.05\% & \cellcolor[HTML]{FFE8A6}52.38\% & \cellcolor[HTML]{5CBD8E}53.08\% & \cellcolor[HTML]{7BCAA3}50.35\% \\ \hline
\end{tabular}
\caption{Linear Kernel Extreme Learning Machine model performance results expressed in UAR. "aro": arousal, "val": valence, "like": likeability.}
\label{tab:lkelm}
\end{table*}

The results of the SVM models trained using the extracted video features are shown in table \ref{tab:svm}. Again, the results are expressed in their UAR, and the highest score for each dimension is highlighted in bold. The results show moderately good performance. The highest scores for each dimension is higher than chance level, but they are slightly worse than the ELM models. 

\begin{table*}[h]
\begin{tabular}{|l|r|r|r|r|r|r|}
\hline
\rowcolor[HTML]{C0C0C0} 
\textbf{Normalization} & \multicolumn{1}{P{1.5cm}|}{\cellcolor[HTML]{C0C0C0}\textbf{self aro}} & \multicolumn{1}{P{1.5cm}|}{\cellcolor[HTML]{C0C0C0}\textbf{gold aro}} & \multicolumn{1}{P{1.5cm}|}{\cellcolor[HTML]{C0C0C0}\textbf{self val}} & \multicolumn{1}{P{1.5cm}|}{\cellcolor[HTML]{C0C0C0}\textbf{gold val}} & \multicolumn{1}{P{1.5cm}|}{\cellcolor[HTML]{C0C0C0}\textbf{self like}} & \multicolumn{1}{P{1.5cm}|}{\cellcolor[HTML]{C0C0C0}\textbf{gold like}} \\ \hline
\textbf{NN} & \cellcolor[HTML]{FFE1E1}42.86\% & \cellcolor[HTML]{FF0000}\textbf{49.05\%} & \cellcolor[HTML]{FFDD7F}49.66\% & \cellcolor[HTML]{FFE7A3}47.92\% & \cellcolor[HTML]{E1F3EA}41.90\% & \cellcolor[HTML]{70C69C}47.25\%  \\ \hline
\textbf{ZN} & \cellcolor[HTML]{FFBBBB}\textbf{43.90\%} & \cellcolor[HTML]{FF2020}48.18\% & \cellcolor[HTML]{FFFFFD}43.57\% & \cellcolor[HTML]{FFFFFF}43.47\% & \cellcolor[HTML]{C3E7D6}\textbf{43.29\%} & \cellcolor[HTML]{57BB8A}\textbf{48.44\%}  \\ \hline
\textbf{L2} & \cellcolor[HTML]{FFD6D6}43.16\% & \cellcolor[HTML]{FF0E0E}48.68\% & \cellcolor[HTML]{FFD666}\textbf{50.83\%} & \cellcolor[HTML]{FFE59D}\textbf{48.22\%} & \cellcolor[HTML]{FFFFFF}40.42\% & \cellcolor[HTML]{9BD7BA}45.20\% \\ \hline
\textbf{ZN+L2} & \cellcolor[HTML]{FFFFFF}42.01\% & \cellcolor[HTML]{FF6D6D}46.06\% & \cellcolor[HTML]{FFF6DB}45.23\% & \cellcolor[HTML]{FFF9E7}44.66\% & \cellcolor[HTML]{F6FCF9}40.86\% & \cellcolor[HTML]{67C295}47.71\%  \\ \hline
\end{tabular}
\caption{Support Vector Machine model performance results expressed in UAR. "aro": arousal, "val": valence, "like": likeability.}
\label{tab:svm}
\end{table*}

Other types of kernels, like the RBF and sigmoid kernel, were explored for the ELM and SVM models. However, these models performed worse on all dimensions compared to the linear models. 

\subsection{Personality Trait experiments}



\begin{table*}[]
\begin{tabular}{|
>{\columncolor[HTML]{EFEFEF}}l |
>{\columncolor[HTML]{EFEFEF}}l |r|r|r|r|r|r|}
\hline
\textbf{Weighted} & \textbf{Normalization} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}\textbf{AGRE}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}\textbf{CONS}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}\textbf{EXTRA}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}\textbf{NEUR}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}\textbf{OPEN}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}\textbf{INTER}} \\ \hline
\textbf{No} & \textbf{NN} & \cellcolor[HTML]{FF8585}60.12\% & \cellcolor[HTML]{FFD86C}66.45\% & \cellcolor[HTML]{9ED8BC}70.90\% & \cellcolor[HTML]{FFFFFF}62.16\% & \cellcolor[HTML]{FFFFFF}64.23\% & \cellcolor[HTML]{D18CFF}67.94\% \\ \hline
\textbf{No} & \textbf{ZN} & \cellcolor[HTML]{FFE0E0}58.79\% & \cellcolor[HTML]{FFE9AC}64.55\% & \cellcolor[HTML]{EEF8F3}69.36\% & \cellcolor[HTML]{3D3DFF}64.42\% & \cellcolor[HTML]{FFC061}69.43\% & \cellcolor[HTML]{CE84FF}68.04\% \\ \hline
\textbf{No} & \textbf{L2} & \cellcolor[HTML]{FF5757}60.79\% & \cellcolor[HTML]{FFD970}66.34\% & \cellcolor[HTML]{6CC499}71.88\% & \cellcolor[HTML]{0000FF}\textbf{65.12\%} & \cellcolor[HTML]{FFCB7D}68.51\% & \cellcolor[HTML]{C977FF}68.19\% \\ \hline
\textbf{No} & \textbf{ZN+L2} & \cellcolor[HTML]{FFFFFF}58.33\% & \cellcolor[HTML]{FFF3D1}63.46\% & \cellcolor[HTML]{FFFFFF}69.02\% & \cellcolor[HTML]{7474FF}63.78\% & \cellcolor[HTML]{FFB546}70.32\% & \cellcolor[HTML]{E1B4FF}67.49\% \\ \hline
\textbf{Yes} & \textbf{NN} & \cellcolor[HTML]{FF0000}\textbf{62.05\%} & \cellcolor[HTML]{FFEBB5}64.29\% & \cellcolor[HTML]{DFF3E9}69.64\% & \cellcolor[HTML]{6464FF}63.97\% & \cellcolor[HTML]{FFB038}70.77\% & \cellcolor[HTML]{FFFFFF}66.62\% \\ \hline
\textbf{Yes} & \textbf{ZN} & \cellcolor[HTML]{FFD5D5}58.95\% & \cellcolor[HTML]{FFF8E4}62.90\% & \cellcolor[HTML]{CDEBDC}70.00\% & \cellcolor[HTML]{3D3DFF}64.42\% & \cellcolor[HTML]{FFB84E}70.05\% & \cellcolor[HTML]{9900FF}\textbf{69.56\%} \\ \hline
\textbf{Yes} & \textbf{L2} & \cellcolor[HTML]{FF8888}60.07\% & \cellcolor[HTML]{FFD666}\textbf{66.61\%} & \cellcolor[HTML]{57BB8A}\textbf{72.28\%} & \cellcolor[HTML]{6868FF}63.92\% & \cellcolor[HTML]{FF9900}\textbf{72.58\%} & \cellcolor[HTML]{EDD0FF}67.16\% \\ \hline
\textbf{Yes} & \textbf{ZN+L2} & \cellcolor[HTML]{FFBFBF}59.27\% & \cellcolor[HTML]{FFFFFF}62.06\% & \cellcolor[HTML]{DEF2E8}69.67\% & \cellcolor[HTML]{3C3CFF}64.43\% & \cellcolor[HTML]{FFC772}68.87\% & \cellcolor[HTML]{AF35FF}68.95\% \\ \hline
\end{tabular}
\caption{Kernel Extreme Learning Machine results for the personality trait and interview variables (binary). AGRE: agreeablenss, CONS: conscientiousness, EXTRA: extraversion, NEUR: neuroticism, OPEN: Openness to Experience, INTER: interiew invitation.}
\label{tab:kelmperson}
\end{table*}