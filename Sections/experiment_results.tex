This section will provide, analyze and discuss the results of the annotations and of the experiments done with the data-set. First, general statistics will be provided regarding the mood and likeability annotations. Then the new annotations in conjunction with the interview variable will be analyzed to identify any statistical relationship. Next, we will provide the results of the experiments. The performance of the models created during the experiments will be expressed in their Unweighted Average Recall or UAR. The UAR is the mean of the recall scores of each class. The recall is the ratio between the number of true positives and false negatives. If all predictions would be correctly classified the UAR would be 1. The UAR metric  is a better indication of prediction capacity than precision accuracy as our data-set is imbalanced. The UAR does not favor the majority class and is therefore a better indicator of the prediction capabilities of our models.
\break

\subsection{Annotation Analysis}
As described in the methodology, 960 video clips of the data-set were annotated for the Big Five personality traits, two mood primitives (valence and arousal), likeability, background music and the interview variable. The personality and interview dimensions were annotated using Amazon Mechanical Turk. All dimensions, with the exception of background music, were annotated for apparent presence or absence. The interview variable indicates whether the annotator would invite the subject to an interview or not. Table \ref{tab:personcount} and table \ref{tab:goldcount} show the number of annotations for each class and dimension. 

The apparent personality trait and interview dimensions were post-processed to create cardinal scores for each video clip (\cite{escalante2018explaining}). These scores were also binarized by taking the mean of each dimension and assigning a 2 if the score was equal or higher than the mean and assigning a 1 if the score was lower than the mean. Table \ref{tab:personcount} shows the number of annotations based on the binarized data-set. 

\begin{table}[h]
\begin{tabular}{|r|r|r|r|}
\hline
\rowcolor[HTML]{C0C0C0} 
\multicolumn{1}{|l|}{\cellcolor[HTML]{C0C0C0}Class} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}Arousal} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}Valence} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}Likeability} \\ \hline
1                           & 77  & 46  & 99  \\
2                           & 364 & 707 & 505 \\
3                           & 519 & 207 & 356 \\ \hline
\multicolumn{1}{|l|}{Total} & 960 & 960 & 960 \\ \hline
\end{tabular}
\caption{Number of annotations (self) per class of the mood primitive and likeability dimensions}
\label{tab:selfcount}
\end{table}

\begin{table}[h]
\begin{tabular}{|r|r|r|r|}
\hline
\rowcolor[HTML]{C0C0C0} 
\multicolumn{1}{|l|}{\cellcolor[HTML]{C0C0C0}Class} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}Arousal} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}Valence} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}Likeability} \\ \hline
1                           & 108 & 82  & 197 \\
2                           & 593 & 705 & 602 \\
3                           & 259 & 173 & 161 \\ \hline
\multicolumn{1}{|l|}{Total} & 960 & 960 & 960 \\ \hline
\end{tabular}
\caption{Number of annotations (gold min) per class of the mood primitive and likeability dimensions}
\label{tab:goldcount}
\end{table}

\begin{table*}[th]
\begin{tabular}{|r|r|r|r|r|r|r|}
\hline
\rowcolor[HTML]{C0C0C0} Class &
  \multicolumn{1}{l|}{Openness} &
  \multicolumn{1}{P{2cm}|}{Conscientiousness} &
  \multicolumn{1}{l|}{Extraversion} &
  \multicolumn{1}{P{2cm}|}{Agreeableness} &
  \multicolumn{1}{l|}{Neuroticism} &
  \multicolumn{1}{P{2cm}|}{Interview Invitation} \\ \hline
1     & 528 & 458 & 443 & 437 & 441 & 426 \\ \hline
2     & 432 & 502 & 517 & 523 & 519 & 534 \\ \hline
Total & 960 & 960 & 960 & 960 & 960 & 960 \\ \hline
\end{tabular}
\caption{Number of annotations per class of apparent personality trait and interview invitation dimensions}
\label{tab:personcount}
\end{table*}

\subsection{Statistical Relationships}

Some interesting statistical experiments can be done regarding the statistical relationship of the mood primitives and likeability dimensions and the interview invitation. For these experiments we will use the cardinal values of the interview variable and the self annotations of the arousal, valence and likeability dimensions. Three box-plots are created and are shown in figure \ref{fig:boxplotval}, figure \ref{fig:boxplotaro} and figure \ref{fig:boxplotlike}. For these box-plots the y-axis indicates the cardinal interview invitation variable, where a higher score means they would be more likely to be invited for an interview. The x-axis indicate the three ordinal classes of the arousal, valence or likeability dimensions. The orange horizontal line in the box-plots indicate the mean. As the plots show, there is a upwards trend in the mean of all three box-plots. Meaning that if the subject is classified in the higher class, they would be more likely to have a higher score in the interview invitation variable.

\begin{figure}[h]
  \centering
  \includesvg[width=\columnwidth]{Images/boxplot_valence_interview.svg}
  \caption{Box-plot of the valence classes and interview invitation variable}
  \label{fig:boxplotval}
\end{figure}

\begin{figure}[h]
  \centering
  \includesvg[width=\columnwidth]{Images/boxplot_arousal_interview.svg}
  \caption{Box-plot of the arousal classes and interview invitation variable}
  \label{fig:boxplotaro}
\end{figure}

\begin{figure}[h]
  \centering
  \includesvg[width=\columnwidth]{Images/boxplot_likeability_interview.svg}
  \caption{Box-plot of the likeability classes and interview invitation variable}
  \label{fig:boxplotlike}
\end{figure}

However, to see if there is a correlation between the interview variable and mood and likeability variables we must look at the Spearman's Rank-Order Correlation coefficient. This coefficient is used to calculate correlation for ordinal categorical variables. To calculate this coefficient we will be using the binarized data-set of the interview variable. The coefficient is calculated as follows: \(\rho = \frac{cov(x,y)}{\sigma_x \sigma_y}\), where \(cov(x,y)\) is the covariance of the variables and \(\rho\) is the Pearson correlation coefficient. Table \ref{tab:spearman} shows the correlation and p value between the three dimensions and the binarized interview variable. The results show a low p value for all dimensions, with every p value being close to 0. This means that the null hypothesis should be rejected, where the null hypothesis is that two sets of data are uncorrelated. By rejected the null hypothesis we can conclude that the sets of data are not uncorrelated, however it does not prove that they are correlated. If we look at table \ref{tab:spearman} we can see that the correlation values are between 0.2 and 0.4. For the arousal dimension this means that the strength of the correlation is small, while for the valence and likeability dimensions there is medium strong correlation. A perfect correlation would be indicated with a value of 1 or -1 (for a negative correlation). Typically, a correlation is considered strong if the correlation coefficient value is higher than 0.7 or lower than -0.7. 

\begin{table}[h]
\begin{tabular}{|P{1cm}|r|r|r|}
\hline
\rowcolor[HTML]{C0C0C0} 
 & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}Arousal} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}Valence} & \multicolumn{1}{l|}{\cellcolor[HTML]{C0C0C0}Likeability} \\ \hline
Correlation & 0.296 & 0.204 & 0.395 \\ \hline
P Value & 6.51E-21 & 1.76E-10 & 3.09E-37 \\ \hline
\end{tabular}
\caption{Results of the Spearman Rank Correlation coefficient calculations}
\label{tab:spearman}
\end{table}

\subsection{Mood and Likeability Model Experiments}\label{subsection:moodlikeexp}
In this section we will discuss the performance of the models that are trained using the mood and likeability dimensions and the extracted video features. As stated in the methodology the data-set was split into a training set of 660 rows and a test set of 300 rows. Both the self annotation and the gold standard annotations were used for training the classifiers. Both the SVM and ELM algorithms as described in section \ref{subsection:classificaiton} were used to the train models. Table \ref{tab:lkelm} shows the results in UAR of the trained ELM models. A linear kernel was used to train these models. Also, hyperparameters, like the C parameter, were optimized to achieve the best scores. The highest score of each dimension is highlighted bold. The normalization column refers to what type of normalization was applied to the feature set. "NN" stands for no normalization, "ZN" for z normalization and "L2" stands for l2 normalization. These are the normalization techniques as described in section \ref{subsection:normalization}. As the results show, the best results of each dimension is higher than 33\%. Which indicates that the models are more accurate than chance level. Chance level is 33\% in the case of our data-set as the dimensions are labeled with three different classes. Further, the results for the weighted ELM are higher than non-weighted ELM for all dimensions. Also, the gold standard annotation dimensions score higher than the self annotated labels. This implies that more annotators are needed for the same data to increase the performance of the models. 

\begin{table*}[h]
\begin{tabular}{|l|P{2cm}|r|r|r|r|r|r|}
\hline
\rowcolor[HTML]{C0C0C0} 
\textbf{Weighted} & \textbf{Normalization} & \multicolumn{1}{P{1.5cm}|}{\cellcolor[HTML]{C0C0C0}\textbf{self aro}} & \multicolumn{1}{P{1.5cm}|}{\cellcolor[HTML]{C0C0C0}\textbf{gold aro}} & \multicolumn{1}{P{1.5cm}|}{\cellcolor[HTML]{C0C0C0}\textbf{self val}} & \multicolumn{1}{P{1.5cm}|}{\cellcolor[HTML]{C0C0C0}\textbf{gold val}} & \multicolumn{1}{P{1.5cm}|}{\cellcolor[HTML]{C0C0C0}\textbf{self like}} & \multicolumn{1}{P{1.5cm}|}{\cellcolor[HTML]{C0C0C0}\textbf{gold like}} \\ \hline
\textbf{No} & \textbf{NN} & \cellcolor[HTML]{FFE2E2}39.79\% & \cellcolor[HTML]{FFAEAE}42.67\% & \cellcolor[HTML]{FFF9E7}47.91\% & \cellcolor[HTML]{FFF9E9}47.78\% & \cellcolor[HTML]{FFFFFF}38.47\% & \cellcolor[HTML]{D1EDDF}42.61\% \\ \hline
\textbf{No} & \textbf{ZN} & \cellcolor[HTML]{FF9B9B}43.76\% & \cellcolor[HTML]{FF2E2E}49.88\% & \cellcolor[HTML]{FFF4D5}49.11\% & \cellcolor[HTML]{FFFFFF}46.21\% & \cellcolor[HTML]{6DC499}51.56\% & \cellcolor[HTML]{88CFAD}49.11\% \\ \hline
\textbf{No} & \textbf{L2} & \cellcolor[HTML]{FFE1E1}39.80\% & \cellcolor[HTML]{FFB2B2}42.49\% & \cellcolor[HTML]{FFFAEC}47.53\% & \cellcolor[HTML]{FFFCF1}47.19\% & \cellcolor[HTML]{F0F9F5}39.84\% & \cellcolor[HTML]{CAEADA}43.25\% \\ \hline
\textbf{No} & \textbf{ZN+L2} & \cellcolor[HTML]{FFE4E4}39.68\% & \cellcolor[HTML]{FF5656}47.63\% & \cellcolor[HTML]{FFF1C9}49.95\% & \cellcolor[HTML]{FFFEF9}46.64\% & \cellcolor[HTML]{8DD1B0}48.68\% & \cellcolor[HTML]{7FCBA6}49.98\% \\ \hline
\textbf{Yes} & \textbf{NN} & \cellcolor[HTML]{FFC1C1}41.60\% & \cellcolor[HTML]{FF5F5F}47.15\% & \cellcolor[HTML]{FFF5D7}49.00\% & \cellcolor[HTML]{FFE8A8}52.25\% & \cellcolor[HTML]{70C59B}51.33\% & \cellcolor[HTML]{5CBE8E}53.04\% \\ \hline
\textbf{Yes} & \textbf{ZN} & \cellcolor[HTML]{FF8383}\textbf{45.13\%} & \cellcolor[HTML]{FF0000}\textbf{52.46\%} & \cellcolor[HTML]{FFE9AD}\textbf{51.92\%} & \cellcolor[HTML]{FFDA73}55.92\% & \cellcolor[HTML]{5BBD8D}\textbf{53.19\%} & \cellcolor[HTML]{86CEAB}49.32\% \\ \hline
\textbf{Yes} & \textbf{L2} & \cellcolor[HTML]{FFFFFF}38.11\% & \cellcolor[HTML]{FF1919}51.08\% & \cellcolor[HTML]{FFF3D2}49.36\% & \cellcolor[HTML]{FFD666}\textbf{56.77\%} & \cellcolor[HTML]{66C195}52.18\% & \cellcolor[HTML]{57BB8A}\textbf{53.49\%} \\ \hline
\textbf{Yes} & \textbf{ZN+L2} & \cellcolor[HTML]{FF9898}43.91\% & \cellcolor[HTML]{FF0202}52.37\% & \cellcolor[HTML]{FFF1C8}50.05\% & \cellcolor[HTML]{FFE8A6}52.38\% & \cellcolor[HTML]{5CBD8E}53.08\% & \cellcolor[HTML]{7BCAA3}50.35\% \\ \hline
\end{tabular}
\caption{Linear Kernel Extreme Learning Machine model performance results expressed in UAR. "aro": arousal, "val": valence, "like": likeability.}
\label{tab:lkelm}
\end{table*}

The results of the SVM models trained using the extracted video features are shown in table \ref{tab:svm}. Again, the results are expressed in their UAR, and the highest score for each dimension is highlighted in bold. The results show moderately good performance. The highest scores for each dimension is higher than chance level, but they are slightly worse than the ELM models. 

\begin{table*}[h]
\begin{tabular}{|l|r|r|r|r|r|r|}
\hline
\rowcolor[HTML]{C0C0C0} 
\textbf{Normalization} & \multicolumn{1}{P{1.5cm}|}{\cellcolor[HTML]{C0C0C0}\textbf{self aro}} & \multicolumn{1}{P{1.5cm}|}{\cellcolor[HTML]{C0C0C0}\textbf{gold aro}} & \multicolumn{1}{P{1.5cm}|}{\cellcolor[HTML]{C0C0C0}\textbf{self val}} & \multicolumn{1}{P{1.5cm}|}{\cellcolor[HTML]{C0C0C0}\textbf{gold val}} & \multicolumn{1}{P{1.5cm}|}{\cellcolor[HTML]{C0C0C0}\textbf{self like}} & \multicolumn{1}{P{1.5cm}|}{\cellcolor[HTML]{C0C0C0}\textbf{gold like}} \\ \hline
\textbf{NN} & \cellcolor[HTML]{FFE1E1}42.86\% & \cellcolor[HTML]{FF0000}\textbf{49.05\%} & \cellcolor[HTML]{FFDD7F}49.66\% & \cellcolor[HTML]{FFE7A3}47.92\% & \cellcolor[HTML]{E1F3EA}41.90\% & \cellcolor[HTML]{70C69C}47.25\%  \\ \hline
\textbf{ZN} & \cellcolor[HTML]{FFBBBB}\textbf{43.90\%} & \cellcolor[HTML]{FF2020}48.18\% & \cellcolor[HTML]{FFFFFD}43.57\% & \cellcolor[HTML]{FFFFFF}43.47\% & \cellcolor[HTML]{C3E7D6}\textbf{43.29\%} & \cellcolor[HTML]{57BB8A}\textbf{48.44\%}  \\ \hline
\textbf{L2} & \cellcolor[HTML]{FFD6D6}43.16\% & \cellcolor[HTML]{FF0E0E}48.68\% & \cellcolor[HTML]{FFD666}\textbf{50.83\%} & \cellcolor[HTML]{FFE59D}\textbf{48.22\%} & \cellcolor[HTML]{FFFFFF}40.42\% & \cellcolor[HTML]{9BD7BA}45.20\% \\ \hline
\textbf{ZN+L2} & \cellcolor[HTML]{FFFFFF}42.01\% & \cellcolor[HTML]{FF6D6D}46.06\% & \cellcolor[HTML]{FFF6DB}45.23\% & \cellcolor[HTML]{FFF9E7}44.66\% & \cellcolor[HTML]{F6FCF9}40.86\% & \cellcolor[HTML]{67C295}47.71\%  \\ \hline
\end{tabular}
\caption{Support Vector Machine model performance results expressed in UAR. "aro": arousal, "val": valence, "like": likeability.}
\label{tab:svm}
\end{table*}

Other types of kernels, like the RBF and sigmoid kernel, were explored for the ELM and SVM models. However, these models performed worse on all dimensions compared to the linear models. 

\subsection{Personality Trait Experiments}\label{subsection:personexp}

In this section, the ELM models are trained to predict the big five personality traits and the interview invitation variable. For this experiment, the same 660 training and 300 test features were used as the mood primitives and likeability model experiments. Also, the binarized version of the big five personality trait and interview variables were used to train the model. The experiment UAR scores are shown in table \ref{tab:kelmperson}, and the highest score for each dimension is highlighted in bold. As the results show, the scores are substantially better than the scores of the mood and likeability dimensions. The best UAR scores are higher than 60\% and some are over 70\%. Overall, the Extraversion and Openness to Experience dimensions achieve the highest UAR scores in our trained models. 

\begin{table*}[h]
\begin{tabular}{|
>{\columncolor[HTML]{EFEFEF}}l |
>{\columncolor[HTML]{EFEFEF}}l |r|r|r|r|r|r|}
\hline
\textbf{Weighted} & \textbf{Normalization} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}\textbf{AGRE}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}\textbf{CONS}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}\textbf{EXTRA}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}\textbf{NEUR}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}\textbf{OPEN}} & \multicolumn{1}{l|}{\cellcolor[HTML]{EFEFEF}\textbf{INTER}} \\ \hline
\textbf{No} & \textbf{NN} & \cellcolor[HTML]{FF8585}60.12\% & \cellcolor[HTML]{FFD86C}66.45\% & \cellcolor[HTML]{9ED8BC}70.90\% & \cellcolor[HTML]{FFFFFF}62.16\% & \cellcolor[HTML]{FFFFFF}64.23\% & \cellcolor[HTML]{D18CFF}67.94\% \\ \hline
\textbf{No} & \textbf{ZN} & \cellcolor[HTML]{FFE0E0}58.79\% & \cellcolor[HTML]{FFE9AC}64.55\% & \cellcolor[HTML]{EEF8F3}69.36\% & \cellcolor[HTML]{3D3DFF}64.42\% & \cellcolor[HTML]{FFC061}69.43\% & \cellcolor[HTML]{CE84FF}68.04\% \\ \hline
\textbf{No} & \textbf{L2} & \cellcolor[HTML]{FF5757}60.79\% & \cellcolor[HTML]{FFD970}66.34\% & \cellcolor[HTML]{6CC499}71.88\% & \cellcolor[HTML]{0000FF}\textbf{65.12\%} & \cellcolor[HTML]{FFCB7D}68.51\% & \cellcolor[HTML]{C977FF}68.19\% \\ \hline
\textbf{No} & \textbf{ZN+L2} & \cellcolor[HTML]{FFFFFF}58.33\% & \cellcolor[HTML]{FFF3D1}63.46\% & \cellcolor[HTML]{FFFFFF}69.02\% & \cellcolor[HTML]{7474FF}63.78\% & \cellcolor[HTML]{FFB546}70.32\% & \cellcolor[HTML]{E1B4FF}67.49\% \\ \hline
\textbf{Yes} & \textbf{NN} & \cellcolor[HTML]{FF0000}\textbf{62.05\%} & \cellcolor[HTML]{FFEBB5}64.29\% & \cellcolor[HTML]{DFF3E9}69.64\% & \cellcolor[HTML]{6464FF}63.97\% & \cellcolor[HTML]{FFB038}70.77\% & \cellcolor[HTML]{FFFFFF}66.62\% \\ \hline
\textbf{Yes} & \textbf{ZN} & \cellcolor[HTML]{FFD5D5}58.95\% & \cellcolor[HTML]{FFF8E4}62.90\% & \cellcolor[HTML]{CDEBDC}70.00\% & \cellcolor[HTML]{3D3DFF}64.42\% & \cellcolor[HTML]{FFB84E}70.05\% & \cellcolor[HTML]{9900FF}\textbf{69.56\%} \\ \hline
\textbf{Yes} & \textbf{L2} & \cellcolor[HTML]{FF8888}60.07\% & \cellcolor[HTML]{FFD666}\textbf{66.61\%} & \cellcolor[HTML]{57BB8A}\textbf{72.28\%} & \cellcolor[HTML]{6868FF}63.92\% & \cellcolor[HTML]{FF9900}\textbf{72.58\%} & \cellcolor[HTML]{EDD0FF}67.16\% \\ \hline
\textbf{Yes} & \textbf{ZN+L2} & \cellcolor[HTML]{FFBFBF}59.27\% & \cellcolor[HTML]{FFFFFF}62.06\% & \cellcolor[HTML]{DEF2E8}69.67\% & \cellcolor[HTML]{3C3CFF}64.43\% & \cellcolor[HTML]{FFC772}68.87\% & \cellcolor[HTML]{AF35FF}68.95\% \\ \hline
\end{tabular}
\caption{Kernel Extreme Learning Machine results for the personality trait and interview variables (binary). AGRE: agreeableness, CONS: conscientiousness, EXTRA: extroversion, NEUR: neuroticism, OPEN: Openness to Experience, INTER: interview invitation.}
\label{tab:kelmperson}
\end{table*}

\subsection{3 Fold Cross Validation}\label{subsection:3foldcv}
The results of the experiments in sections \ref{subsection:moodlikeexp} and \ref{subsection:personexp} show that the models can reach an accuracy level of around 55\% for the mood and likeability dimensions and upward of 72\% for the binary personality trait and interview dimensions. These models were optimized by utilizing weighted models, feature normalization and hyperparamater optimization. During the process of hyperparameter optimization parameters like the C value are optimized to increase performance. However, this means that the model is optimized for the test set, as this set is used to compute the performance score. This means that the performance of the model might not be optimal. To solve this problem the k fold cross validation method can be applied. In our case we did a 3 fold cross validation which means that our original 660 instances of our training set was split up in a new train set of 440 samples and test set of 220 samples. The split was done three times to train and test three models per dimension. Then the optimal hyperparameters were chosen based on the highest performance scores of each model. The prediction output of the test sets were combined to create a data-set of 660 high level features. These instances were also compared with the original labels to compute the UAR, which can be found in table \ref{tab:3foldtestuar} in the column of "3 Fold CV". "Weighted" refers to if the weighted version of the ELM classifier was used or not. Next, the last 300 video features were deployed on the optimal models that were trained during the 3 Fold Cross Validation. This resulted in 3 times 300 predictions per dimension. Using majority voting, or choosing the medium class in case of a draw, the final prediction set was constructed for each dimension. These predictions were also compared to the original labels to compute the UAR. The scores can be found in table \ref{tab:3foldtestuar} in the "Testset" column. Both L2 and ZN normalization was applied to the video feature set to train the models in the cross validations. 



\begin{table}[h]
\begin{tabular}{|l|r|r|r|r|}
\hline
 & \multicolumn{2}{l|}{3 Fold CV} & \multicolumn{2}{l|}{Test set} \\ \hline
Weighted & \multicolumn{1}{l|}{Yes} & \multicolumn{1}{l|}{No} & \multicolumn{1}{l|}{Yes} & \multicolumn{1}{l|}{No} \\ \hline
INTER & 65.75\% & 67.05\% & 66.87\% & 66.75\% \\ \hline
AGRE & 62.67\% & 61.68\% & 59.35\% & 58.95\% \\ \hline
OPEN & 67.64\% & 68.23\% & 70.34\% & 67.65\% \\ \hline
NEUR & 65.71\% & 66.29\% & 60.38\% & 59.36\% \\ \hline
EXTRA & 70.62\% & 70.07\% & 71.02\% & 69.35\% \\ \hline
CONST & 65.20\% & 65.36\% & 59.39\% & 58.56\% \\ \hline
ARO & 47.67\% & 41.38\% & 44.25\% & 37.26\% \\ \hline
VAL & 56.17\% & 52.74\% & 43.38\% & 37.21\% \\ \hline
LIKE & 45.97\% & 43.16\% & 43.95\% & 42.54\% \\ \hline
\end{tabular}
\caption{The UAR scores of the 3 Fold Cross Validation experiment and the predicted test set using ELM.}
\label{tab:3foldtestuar}
\end{table}

\subsection{Explainability Analysis and Decision Trees}
The performance results of the trained models are overall good, as they show UAR scores of around 65\%. Now to explain the predictions made by our model we will be training and visualizing a decision tree. However, a decision tree trained with over 16000 video features would result in a tree with over 2\textsuperscript{16000} branches. Therefore, to increase explainability, we will choose 5 to 8 high level features to model the interview invitation variable. We will use the predictions of the 3 fold cross validation models, as described in section \ref{subsection:3foldcv}, as the high level features. Meaning, the openness, conscientiousness, extroversion, agreeableness, neuroticism, valence, arousal and likeability predictions were used as the high level features. Meanwhile, the interview dimension was used as the label. 

Table \ref{tab:dtuar} show the UAR scores of the trained decision tree models for each dimension combination. Personality refers to the big five personality traits, while weighted and unweighted refers to the type of prediction set that was used. Note that since the predicted dimension is binary, the chance level is 50\%. To increase the explainability and interpretability of the decision tree, the mood and likability dimensions were, like the personality traits, also binarized. The mood and likeability dimensions had few samples for the low class so the decision was to combine them with the medium class.

\begin{table*}[]
\begin{tabular}{|l|r|r|}
\hline
 & \multicolumn{1}{l|}{Weighted} & \multicolumn{1}{l|}{Unweighted} \\ \hline
Personality & \cellcolor[HTML]{86CEAB}63.86\% & \cellcolor[HTML]{D1EDDF}63.87\% \\ \hline
Personality + valence & \cellcolor[HTML]{CBEADB}62.83\% & \cellcolor[HTML]{FCFEFD}63.20\% \\ \hline
Personality + arousal & \cellcolor[HTML]{E3F4EC}62.47\% & \cellcolor[HTML]{83CDA9}65.10\% \\ \hline
Personality + likeability & 61.68\% & 64.80\% \\ \hline
Personality + valence + arousal & \cellcolor[HTML]{FFFFFF}62.04\% & \cellcolor[HTML]{57BB8A}\textbf{65.78\%} \\ \hline
Personality + valence + likeability & 63.26\% & \cellcolor[HTML]{D1EDDF}63.87\% \\ \hline
Personality + likeability+ arousal & \cellcolor[HTML]{57BB8A}\textbf{64.56\%} & \cellcolor[HTML]{F0F9F5}63.39\% \\ \hline
Personality + valence + arousal + likeability & \cellcolor[HTML]{5CBD8D}64.50\% & \cellcolor[HTML]{FFFFFF}63.15\% \\ \hline
\end{tabular}
\caption{UAR scores of the modeled Decision Trees per dimension combination.}
\label{tab:dtuar}
\end{table*}

As the results indicate for the unweighted predictors, adding arousal to the dimensions increases the performance score of the models, except when adding the likeability dimension. For the weighted prediction set, the likeability and arousal combination was the best scorer. The overall top scorer, with a UAR score close to 66\%, used the unweighted prediction set of the personality traits, valence and arousal dimensions. 

Figure \ref{fig:decisiontree} is the visualization of the decision tree which has the highest UAR score. The constructed decision tree can be interpreted as follows: a node will contain a high level feature, agreeableness for example. Then the branch to the left of this node means the feature scored low while the right branch means the feature scored high. The leaf nodes indicate the predicted class, in our case whether or not a person is invited to a job interview or not. Neuroticism correlates negatively with the interview variable, therefore it is represented with its opposite; non-neuroticism. In the decision tree this means that the right branch indicates a high score in non-neuroticism. 
\begin{figure*}[h]
  \centering
  \includesvg[width=\textwidth]{Images/decision_tree.svg}
  \caption{Decision Tree of the personality traits, arousal and valence dimensions, predicting the interview invitation variable.}
  \label{fig:decisiontree}
\end{figure*}

\begin{figure}[h]
  \centering
  \includesvg[width=\columnwidth]{Images/feature_importance_weighted.svg}
  \caption{Feature importance distribution using a Ridge Classifier with the weighted prediction set.}
  \label{fig:featureimp_weighted}
\end{figure}

\begin{figure}[h]
  \centering
  \includesvg[width=\columnwidth]{Images/feature_importance_unweighted.svg}
  \caption{Feature importance distribution using a Ridge Classifier with the unweighted prediction set.}
  \label{fig:featureimp_unweighted}
\end{figure}